{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\" draggable=‚Äùfalse‚Äù ><img src=\"https://user-images.githubusercontent.com/37101144/161836199-fdb0219d-0361-4988-bf26-48b0fad160a3.png\"\n",
    "     width=\"200px\"\n",
    "     height=\"auto\"/>\n",
    "</p>\n",
    "\n",
    "# Week 1 - End-to-end Deployment using Hugging Face and Sagemaker\n",
    "\n",
    "### üõçÔ∏è Hugging Face and Sagemaker\n",
    "\n",
    "Hugging Face is the creator of Transformers, the leading open-source library for building state-of-the-art machine learning models. Hugging Face allows you to choose from tens of thousands of machine learning models for Natural Language Processing, Audio, and Computer Vision, publicly available in the Hugging Face Hub, to accelerate your machine learning workload.\n",
    "\n",
    "Amazon SageMaker is a fully managed machine learning service. With SageMaker, data scientists and developers can quickly and easily build and train machine learning models, and then directly deploy them into a production-ready hosted environment. It provides an integrated Jupyter authoring notebook instance for easy access to your data sources for exploration and analysis, so you don't have to manage servers. It also provides common machine learning algorithms that are optimized to run efficiently against extremely large data in a distributed environment. You can deploy a model into a secure and scalable environment by launching it with a few clicks from SageMaker Studio or the SageMaker console.\n",
    "\n",
    "One Command is All you Need!\n",
    "\n",
    "With the new Hugging Face Deep Learning Containers available in Amazon SageMaker, training cutting-edge Transformers-based NLP models is much easier. There are variants specially optimized for TensorFlow and PyTorch, for single-GPU, single-node multi-GPU and multi-node clusters.\n",
    "\n",
    "In this session, you will learn how to use Amazon SageMaker to train a Hugging Face Transformer model and deploy it afterwards.\n",
    "\n",
    "\n",
    "### üìö Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "- Prepare and upload a test dataset to AWS S3\n",
    "- Prepare a fine-tuning script to be used with Amazon SageMaker Training jobs\n",
    "- Launch a training job and store the trained model into S3\n",
    "- Deploy the model after successful training\n",
    "\n",
    "### üìù Note.\n",
    "In this session, not all the imports are provided; you may need to import necessary modules / functions to be able to run the code successfully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Development Environment and Permissions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "_*Note:* we only install the required libraries from Hugging Face and AWS. You also need PyTorch or Tensorflow, if you haven¬¥t it installed_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xd/x237m6993dq6r5zn23zrcb380000gn/T/ipykernel_38352/3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"sagemaker>=2.48.0\" \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_If you are going to use Sagemaker in a local environment. You need access to an IAM Role with the required permissions for Sagemaker. You can find [here](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-roles.html) more about it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! export PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python\n",
    "# ! pip install protobuf==3.20.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker bucket: yochickenbuttttt1\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = 'yochickenbuttttt1'\n",
    "\n",
    "\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "# print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "We are using the `datasets` library to download and preprocess the `emotion` dataset. After preprocessing, the dataset will be uploaded to our `sagemaker_session_bucket` to be used within our training job. The [emotion](https://github.com/dair-ai/emotion_dataset) dataset consists of 16000 training examples, 2000 validation examples, and 2000 testing examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer used in preprocessing\n",
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "\n",
    "# dataset used\n",
    "dataset_name = 'emotion'\n",
    "\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'samples/datasets/emotion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset emotion (/Users/mattobrien415/.cache/huggingface/datasets/emotion/default/0.0.0/6e4212efe64fd33728549b8f0435c73081391d543b596a05936857df98acb681)\n",
      "Parameter 'function'=<function tokenize at 0x7f86803f6310> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16/16 [00:02<00:00,  6.24ba/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  7.27ba/s]\n"
     ]
    }
   ],
   "source": [
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# load dataset\n",
    "train_dataset, test_dataset = load_dataset(dataset_name, split=['train', 'test'])\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# set format for pytorch\n",
    "train_dataset =  train_dataset.rename_column(\"label\", \"labels\")\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset = test_dataset.rename_column(\"label\", \"labels\")\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uploading data to `sagemaker_session_bucket`\n",
    "\n",
    "After we processed the `datasets` we are going to use the new `FileSystem` [integration](https://huggingface.co/docs/datasets/filesystems.html) to upload our dataset to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(training_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()  \n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(test_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning & starting Sagemaker Training Job\n",
    "\n",
    "In order to create a sagemaker training job we need an `HuggingFace` Estimator. The Estimator handles end-to-end Amazon SageMaker training and deployment tasks. In a Estimator we define, which fine-tuning script should be used as `entry_point`, which `instance_type` should be used, which `hyperparameters` are passed in .....\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "huggingface_estimator = HuggingFace(entry_point='train.py',\n",
    "                            source_dir='./scripts',\n",
    "                            base_job_name='huggingface-sdk-extension',\n",
    "                            instance_type='ml.p3.2xlarge',\n",
    "                            instance_count=1,\n",
    "                            transformers_version='4.4',\n",
    "                            pytorch_version='1.6',\n",
    "                            py_version='py36',\n",
    "                            role=role,\n",
    "                            hyperparameters = {'epochs': 1,\n",
    "                                               'train_batch_size': 32,\n",
    "                                               'model_name':'distilbert-base-uncased'\n",
    "                                                })\n",
    "```\n",
    "\n",
    "When we create a SageMaker training job, SageMaker takes care of starting and managing all the required ec2 instances for us with the `huggingface` container, uploads the provided fine-tuning script `train.py` and downloads the data from our `sagemaker_session_bucket` into the container at `/opt/ml/input/data`. Then, it starts the training job by running. \n",
    "\n",
    "```python\n",
    "/opt/conda/bin/python train.py --epochs 1 --model_name distilbert-base-uncased --train_batch_size 32\n",
    "```\n",
    "\n",
    "The `hyperparameters` you define in the `HuggingFace` estimator are passed in as named arguments. \n",
    "\n",
    "Sagemaker is providing useful properties about the training environment through various environment variables, including the following:\n",
    "\n",
    "* `SM_MODEL_DIR`: A string that represents the path where the training job writes the model artifacts to. After training, artifacts in this directory are uploaded to S3 for model hosting.\n",
    "\n",
    "* `SM_NUM_GPUS`: An integer representing the number of GPUs available to the host.\n",
    "\n",
    "* `SM_CHANNEL_XXXX:` A string that represents the path to the directory that contains the input data for the specified channel. For example, if you specify two input channels in the HuggingFace estimator‚Äôs fit call, named `train` and `test`, the environment variables `SM_CHANNEL_TRAIN` and `SM_CHANNEL_TEST` are set.\n",
    "\n",
    "\n",
    "To run your training job locally you can define `instance_type='local'` or `instance_type='local_gpu'` for gpu usage. _Note: this does not working within SageMaker Studio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msys\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtorch\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mdatasets\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_from_disk, load_metric\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mtrainer_utils\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m get_last_checkpoint\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# hyperparameters sent by the client are passed as command-line arguments to the script.\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m3\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m32\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--eval_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--warmup_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m500\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_id\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34m5e-5\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--fp16\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mbool\u001b[39;49;00m, default=\u001b[34mTrue\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Data, model, and output directories\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output_data_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--n_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--training_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TEST\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "\n",
      "    args, _ = parser.parse_known_args()\n",
      "\n",
      "    \u001b[37m# Set up logging\u001b[39;49;00m\n",
      "    logger = logging.getLogger(\u001b[31m__name__\u001b[39;49;00m)\n",
      "\n",
      "    logging.basicConfig(\n",
      "        level=logging.getLevelName(\u001b[33m\"\u001b[39;49;00m\u001b[33mINFO\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m),\n",
      "        handlers=[logging.StreamHandler(sys.stdout)],\n",
      "        \u001b[36mformat\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33m%(asctime)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(name)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(levelname)s\u001b[39;49;00m\u001b[33m - \u001b[39;49;00m\u001b[33m%(message)s\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# load datasets\u001b[39;49;00m\n",
      "    train_dataset = load_from_disk(args.training_dir)\n",
      "    test_dataset = load_from_disk(args.test_dir)\n",
      "\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m loaded train_dataset length is: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mlen\u001b[39;49;00m(train_dataset)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    logger.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m loaded test_dataset length is: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mlen\u001b[39;49;00m(test_dataset)\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    metric = load_metric(\u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[34mdef\u001b[39;49;00m \u001b[32mcompute_metrics\u001b[39;49;00m(eval_pred):\n",
      "        predictions, labels = eval_pred\n",
      "        predictions = np.argmax(predictions, axis=\u001b[34m1\u001b[39;49;00m)\n",
      "        \u001b[34mreturn\u001b[39;49;00m metric.compute(predictions=predictions, references=labels)\n",
      "\n",
      "    \u001b[37m# Prepare model labels - useful in inference API\u001b[39;49;00m\n",
      "    labels = train_dataset.features[\u001b[33m\"\u001b[39;49;00m\u001b[33mlabels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m].names\n",
      "    num_labels = \u001b[36mlen\u001b[39;49;00m(labels)\n",
      "    label2id, id2label = \u001b[36mdict\u001b[39;49;00m(), \u001b[36mdict\u001b[39;49;00m()\n",
      "    \u001b[34mfor\u001b[39;49;00m i, label \u001b[35min\u001b[39;49;00m \u001b[36menumerate\u001b[39;49;00m(labels):\n",
      "        label2id[label] = \u001b[36mstr\u001b[39;49;00m(i)\n",
      "        id2label[\u001b[36mstr\u001b[39;49;00m(i)] = label\n",
      "\n",
      "    \u001b[37m# download model from model hub\u001b[39;49;00m\n",
      "    model = AutoModelForSequenceClassification.from_pretrained(\n",
      "        args.model_id, num_labels=num_labels, label2id=label2id, id2label=id2label\n",
      "    )\n",
      "    tokenizer = AutoTokenizer.from_pretrained(args.model_id)\n",
      "\n",
      "    \u001b[37m# define training args\u001b[39;49;00m\n",
      "    training_args = TrainingArguments(\n",
      "        output_dir=args.output_dir,\n",
      "        overwrite_output_dir=\u001b[34mTrue\u001b[39;49;00m \u001b[34mif\u001b[39;49;00m get_last_checkpoint(args.output_dir) \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m \u001b[34melse\u001b[39;49;00m \u001b[34mFalse\u001b[39;49;00m,\n",
      "        num_train_epochs=args.epochs,\n",
      "        per_device_train_batch_size=args.train_batch_size,\n",
      "        per_device_eval_batch_size=args.eval_batch_size,\n",
      "        warmup_steps=args.warmup_steps,\n",
      "        fp16=args.fp16,\n",
      "        evaluation_strategy=\u001b[33m\"\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        save_strategy=\u001b[33m\"\u001b[39;49;00m\u001b[33mepoch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        save_total_limit=\u001b[34m2\u001b[39;49;00m,\n",
      "        logging_dir=\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.output_data_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/logs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "        learning_rate=\u001b[36mfloat\u001b[39;49;00m(args.learning_rate),\n",
      "        load_best_model_at_end=\u001b[34mTrue\u001b[39;49;00m,\n",
      "        metric_for_best_model=\u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# create Trainer instance\u001b[39;49;00m\n",
      "    trainer = Trainer(\n",
      "        model=model,\n",
      "        args=training_args,\n",
      "        compute_metrics=compute_metrics,\n",
      "        train_dataset=train_dataset,\n",
      "        eval_dataset=test_dataset,\n",
      "        tokenizer=tokenizer,\n",
      "    )\n",
      "\n",
      "    \u001b[37m# train model\u001b[39;49;00m\n",
      "    \u001b[34mif\u001b[39;49;00m get_last_checkpoint(args.output_dir) \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m:\n",
      "        logger.info(\u001b[33m\"\u001b[39;49;00m\u001b[33m***** continue training *****\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        last_checkpoint = get_last_checkpoint(args.output_dir)\n",
      "        trainer.train(resume_from_checkpoint=last_checkpoint)\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        trainer.train()\n",
      "\n",
      "    \u001b[37m# evaluate model\u001b[39;49;00m\n",
      "    eval_result = trainer.evaluate(eval_dataset=test_dataset)\n",
      "\n",
      "    \u001b[37m# writes eval result to file which can be accessed later in s3 ouput\u001b[39;49;00m\n",
      "    \u001b[34mwith\u001b[39;49;00m \u001b[36mopen\u001b[39;49;00m(os.path.join(args.output_data_dir, \u001b[33m\"\u001b[39;49;00m\u001b[33meval_results.txt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), \u001b[33m\"\u001b[39;49;00m\u001b[33mw\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m writer:\n",
      "        \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m***** Eval results *****\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "        \u001b[34mfor\u001b[39;49;00m key, value \u001b[35min\u001b[39;49;00m \u001b[36msorted\u001b[39;49;00m(eval_result.items()):\n",
      "            writer.write(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mkey\u001b[33m}\u001b[39;49;00m\u001b[33m = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mvalue\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "            \u001b[36mprint\u001b[39;49;00m(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mkey\u001b[33m}\u001b[39;49;00m\u001b[33m = \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mvalue\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Saves the model to s3 uses os.environ[\"SM_MODEL_DIR\"] to make sure checkpointing works\u001b[39;49;00m\n",
      "    trainer.save_model(os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./scripts/train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating an Estimator and start a training job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of supported models: https://huggingface.co/models?library=pytorch,transformers&sort=downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "import time\n",
    "\n",
    "# hyperparameters, which are passed into the training job\n",
    "hyperparameters={'epochs': 1,                          # number of training epochs\n",
    "                 'train_batch_size': 32,               # batch size for training\n",
    "                 'eval_batch_size': 64,                # batch size for evaluation\n",
    "                 'learning_rate': 3e-5,                # learning rate used during training\n",
    "                 'model_id':'distilbert-base-uncased', # pre-trained model\n",
    "                 'fp16': True,                         # Whether to use 16-bit (mixed) precision training\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Training Job Name \n",
    "job_name = f'huggingface-workshop-{time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.localtime())}'\n",
    "\n",
    "\n",
    "# role = 'AmazonSageMaker-ExecutionPolicy-20220420T205328'\n",
    "\n",
    "# role = 'arn:aws:iam::681261969843:policy/service-role/AmazonSageMaker-ExecutionPolicy-20220420T205328'\n",
    "\n",
    "role = 'AmazonSageMaker-ExecutionRole-20220420T205328'\n",
    "\n",
    "# role = 'AmazonSageMakerFullAccess'\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point          = 'train.py',        # fine-tuning script used in training jon\n",
    "    source_dir           = './scripts',       # directory where fine-tuning script is stored\n",
    "    instance_type        = 'ml.p3.2xlarge',   # instances type used for the training job\n",
    "    instance_count       = 1,                 # the number of instances used for training\n",
    "    base_job_name        = job_name,          # the name of the training job\n",
    "    role                 = role,              # Iam role used in training job to access AWS ressources, e.g. S3\n",
    "    transformers_version = '4.6.1',           # the transformers version used in the training job\n",
    "    pytorch_version      = '1.7.1',           # the pytorch_version version used in the training job\n",
    "    py_version           = 'py36',            # the python version used in the training job\n",
    "    hyperparameters      = hyperparameters,   # the hyperparameter used for running the training job\n",
    ")\n",
    "\n",
    "\n",
    "# define a data input dictonary with our uploaded s3 uris\n",
    "data = {\n",
    "    'train': training_input_path,\n",
    "'test': test_input_path\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-09-22 21:14:44 Starting - Starting the training job...\n",
      "2022-09-22 21:15:10 Starting - Preparing the instances for trainingProfilerReport-1663881222: InProgress\n",
      ".........\n",
      "2022-09-22 21:16:51 Downloading - Downloading input data\n",
      "2022-09-22 21:16:51 Training - Downloading the training image..................\n",
      "2022-09-22 21:19:52 Training - Training image download completed. Training in progress.bash: cannot set terminal process group (-1): Inappropriate ioctl for device\n",
      "bash: no job control in this shell\n",
      "2022-09-22 21:19:54,992 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\n",
      "2022-09-22 21:19:55,026 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\n",
      "2022-09-22 21:19:55,034 sagemaker_pytorch_container.training INFO     Invoking user training script.\n",
      "2022-09-22 21:19:55,548 sagemaker-training-toolkit INFO     Invoking user script\n",
      "Training Env:\n",
      "{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 1,\n",
      "        \"eval_batch_size\": 64,\n",
      "        \"fp16\": true,\n",
      "        \"learning_rate\": 3e-05,\n",
      "        \"model_id\": \"distilbert-base-uncased\",\n",
      "        \"train_batch_size\": 32\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-workshop-2022-09-22-15-13-3-2022-09-22-21-13-40-790\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-681261969843/huggingface-workshop-2022-09-22-15-13-3-2022-09-22-21-13-40-790/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p3.2xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p3.2xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\n",
      "}\n",
      "Environment variables:\n",
      "SM_HOSTS=[\"algo-1\"]\n",
      "SM_NETWORK_INTERFACE_NAME=eth0\n",
      "SM_HPS={\"epochs\":1,\"eval_batch_size\":64,\"fp16\":true,\"learning_rate\":3e-05,\"model_id\":\"distilbert-base-uncased\",\"train_batch_size\":32}\n",
      "SM_USER_ENTRY_POINT=train.py\n",
      "SM_FRAMEWORK_PARAMS={}\n",
      "SM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"}\n",
      "SM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\n",
      "SM_OUTPUT_DATA_DIR=/opt/ml/output/data\n",
      "SM_CHANNELS=[\"test\",\"train\"]\n",
      "SM_CURRENT_HOST=algo-1\n",
      "SM_MODULE_NAME=train\n",
      "SM_LOG_LEVEL=20\n",
      "SM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\n",
      "SM_INPUT_DIR=/opt/ml/input\n",
      "SM_INPUT_CONFIG_DIR=/opt/ml/input/config\n",
      "SM_OUTPUT_DIR=/opt/ml/output\n",
      "SM_NUM_CPUS=8\n",
      "SM_NUM_GPUS=1\n",
      "SM_MODEL_DIR=/opt/ml/model\n",
      "SM_MODULE_DIR=s3://sagemaker-us-west-2-681261969843/huggingface-workshop-2022-09-22-15-13-3-2022-09-22-21-13-40-790/source/sourcedir.tar.gz\n",
      "SM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":1,\"eval_batch_size\":64,\"fp16\":true,\"learning_rate\":3e-05,\"model_id\":\"distilbert-base-uncased\",\"train_batch_size\":32},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-workshop-2022-09-22-15-13-3-2022-09-22-21-13-40-790\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-681261969843/huggingface-workshop-2022-09-22-15-13-3-2022-09-22-21-13-40-790/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p3.2xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p3.2xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\n",
      "SM_USER_ARGS=[\"--epochs\",\"1\",\"--eval_batch_size\",\"64\",\"--fp16\",\"True\",\"--learning_rate\",\"3e-05\",\"--model_id\",\"distilbert-base-uncased\",\"--train_batch_size\",\"32\"]\n",
      "SM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\n",
      "SM_CHANNEL_TEST=/opt/ml/input/data/test\n",
      "SM_CHANNEL_TRAIN=/opt/ml/input/data/train\n",
      "SM_HP_EPOCHS=1\n",
      "SM_HP_EVAL_BATCH_SIZE=64\n",
      "SM_HP_FP16=true\n",
      "SM_HP_LEARNING_RATE=3e-05\n",
      "SM_HP_MODEL_ID=distilbert-base-uncased\n",
      "SM_HP_TRAIN_BATCH_SIZE=32\n",
      "PYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "Invoking script with the following command:\n",
      "/opt/conda/bin/python3.6 train.py --epochs 1 --eval_batch_size 64 --fp16 True --learning_rate 3e-05 --model_id distilbert-base-uncased --train_batch_size 32\n",
      "2022-09-22 21:20:00,580 - __main__ - INFO -  loaded train_dataset length is: 16000\n",
      "2022-09-22 21:20:00,580 - __main__ - INFO -  loaded test_dataset length is: 2000\n",
      "2022-09-22 21:20:01,905 - filelock - INFO - Lock 139845419780920 acquired on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\n",
      "2022-09-22 21:20:02,174 - filelock - INFO - Lock 139845419780920 released on /root/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333.lock\n",
      "2022-09-22 21:20:02,453 - filelock - INFO - Lock 139845409415064 acquired on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\n",
      "2022-09-22 21:20:07,601 - filelock - INFO - Lock 139845409415064 released on /root/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a.lock\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2022-09-22 21:20:08,851 - filelock - INFO - Lock 139845276282664 acquired on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "2022-09-22 21:20:09,371 - filelock - INFO - Lock 139845276282664 released on /root/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99.lock\n",
      "2022-09-22 21:20:09,628 - filelock - INFO - Lock 139848594830112 acquired on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "2022-09-22 21:20:10,234 - filelock - INFO - Lock 139848594830112 released on /root/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4.lock\n",
      "2022-09-22 21:20:11,051 - filelock - INFO - Lock 139845276282160 acquired on /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "2022-09-22 21:20:11,316 - filelock - INFO - Lock 139845276282160 released on /root/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79.lock\n",
      "[2022-09-22 21:20:16.064 algo-1:27 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2022-09-22 21:20:16.231 algo-1:27 INFO profiler_config_parser.py:102] User has disabled profiler.\n",
      "[2022-09-22 21:20:16.231 algo-1:27 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\n",
      "[2022-09-22 21:20:16.232 algo-1:27 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\n",
      "[2022-09-22 21:20:16.233 algo-1:27 INFO hook.py:255] Saving to /opt/ml/output/tensors\n",
      "[2022-09-22 21:20:16.234 algo-1:27 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\n",
      "[2022-09-22 21:20:16.426 algo-1:27 INFO hook.py:591] name:distilbert.embeddings.word_embeddings.weight count_params:23440896\n",
      "[2022-09-22 21:20:16.426 algo-1:27 INFO hook.py:591] name:distilbert.embeddings.position_embeddings.weight count_params:393216\n",
      "[2022-09-22 21:20:16.426 algo-1:27 INFO hook.py:591] name:distilbert.embeddings.LayerNorm.weight count_params:768\n",
      "[2022-09-22 21:20:16.426 algo-1:27 INFO hook.py:591] name:distilbert.embeddings.LayerNorm.bias count_params:768\n",
      "[2022-09-22 21:20:16.427 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.q_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.427 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.q_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.427 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.k_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.427 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.k_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.427 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.v_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.427 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.v_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.428 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.out_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.428 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.attention.out_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.428 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.sa_layer_norm.weight count_params:768\n",
      "[2022-09-22 21:20:16.428 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.sa_layer_norm.bias count_params:768\n",
      "[2022-09-22 21:20:16.428 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin1.weight count_params:2359296\n",
      "[2022-09-22 21:20:16.428 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin1.bias count_params:3072\n",
      "[2022-09-22 21:20:16.428 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin2.weight count_params:2359296\n",
      "[2022-09-22 21:20:16.429 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.ffn.lin2.bias count_params:768\n",
      "[2022-09-22 21:20:16.429 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.output_layer_norm.weight count_params:768\n",
      "[2022-09-22 21:20:16.429 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.0.output_layer_norm.bias count_params:768\n",
      "[2022-09-22 21:20:16.429 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.q_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.429 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.q_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.429 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.k_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.430 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.k_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.430 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.v_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.430 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.v_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.431 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.out_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.431 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.attention.out_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.431 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.sa_layer_norm.weight count_params:768\n",
      "[2022-09-22 21:20:16.431 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.sa_layer_norm.bias count_params:768\n",
      "[2022-09-22 21:20:16.432 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin1.weight count_params:2359296\n",
      "[2022-09-22 21:20:16.432 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin1.bias count_params:3072\n",
      "[2022-09-22 21:20:16.432 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin2.weight count_params:2359296\n",
      "[2022-09-22 21:20:16.432 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.ffn.lin2.bias count_params:768\n",
      "[2022-09-22 21:20:16.432 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.output_layer_norm.weight count_params:768\n",
      "[2022-09-22 21:20:16.432 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.1.output_layer_norm.bias count_params:768\n",
      "[2022-09-22 21:20:16.433 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.q_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.433 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.q_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.433 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.k_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.433 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.k_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.433 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.v_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.433 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.v_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.433 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.out_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.433 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.attention.out_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.434 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.sa_layer_norm.weight count_params:768\n",
      "[2022-09-22 21:20:16.434 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.sa_layer_norm.bias count_params:768\n",
      "[2022-09-22 21:20:16.434 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin1.weight count_params:2359296\n",
      "[2022-09-22 21:20:16.434 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin1.bias count_params:3072\n",
      "[2022-09-22 21:20:16.434 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin2.weight count_params:2359296\n",
      "[2022-09-22 21:20:16.434 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.ffn.lin2.bias count_params:768\n",
      "[2022-09-22 21:20:16.434 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.output_layer_norm.weight count_params:768\n",
      "[2022-09-22 21:20:16.434 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.2.output_layer_norm.bias count_params:768\n",
      "[2022-09-22 21:20:16.434 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.q_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.435 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.q_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.435 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.k_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.435 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.k_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.435 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.v_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.435 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.v_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.436 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.out_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.436 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.attention.out_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.436 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.sa_layer_norm.weight count_params:768\n",
      "[2022-09-22 21:20:16.436 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.sa_layer_norm.bias count_params:768\n",
      "[2022-09-22 21:20:16.436 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin1.weight count_params:2359296\n",
      "[2022-09-22 21:20:16.436 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin1.bias count_params:3072\n",
      "[2022-09-22 21:20:16.436 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin2.weight count_params:2359296\n",
      "[2022-09-22 21:20:16.436 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.ffn.lin2.bias count_params:768\n",
      "[2022-09-22 21:20:16.437 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.output_layer_norm.weight count_params:768\n",
      "[2022-09-22 21:20:16.437 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.3.output_layer_norm.bias count_params:768\n",
      "[2022-09-22 21:20:16.437 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.q_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.437 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.q_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.437 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.k_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.437 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.k_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.438 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.v_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.438 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.v_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.438 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.out_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.438 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.attention.out_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.438 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.sa_layer_norm.weight count_params:768\n",
      "[2022-09-22 21:20:16.438 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.sa_layer_norm.bias count_params:768\n",
      "[2022-09-22 21:20:16.438 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin1.weight count_params:2359296\n",
      "[2022-09-22 21:20:16.438 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin1.bias count_params:3072\n",
      "[2022-09-22 21:20:16.438 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin2.weight count_params:2359296\n",
      "[2022-09-22 21:20:16.439 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.ffn.lin2.bias count_params:768\n",
      "[2022-09-22 21:20:16.439 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.output_layer_norm.weight count_params:768\n",
      "[2022-09-22 21:20:16.439 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.4.output_layer_norm.bias count_params:768\n",
      "[2022-09-22 21:20:16.439 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.q_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.439 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.q_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.439 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.k_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.440 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.k_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.440 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.v_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.440 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.v_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.440 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.out_lin.weight count_params:589824\n",
      "[2022-09-22 21:20:16.440 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.attention.out_lin.bias count_params:768\n",
      "[2022-09-22 21:20:16.440 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.sa_layer_norm.weight count_params:768\n",
      "[2022-09-22 21:20:16.440 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.sa_layer_norm.bias count_params:768\n",
      "[2022-09-22 21:20:16.440 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin1.weight count_params:2359296\n",
      "[2022-09-22 21:20:16.440 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin1.bias count_params:3072\n",
      "[2022-09-22 21:20:16.441 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin2.weight count_params:2359296\n",
      "[2022-09-22 21:20:16.441 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.ffn.lin2.bias count_params:768\n",
      "[2022-09-22 21:20:16.441 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.output_layer_norm.weight count_params:768\n",
      "[2022-09-22 21:20:16.441 algo-1:27 INFO hook.py:591] name:distilbert.transformer.layer.5.output_layer_norm.bias count_params:768\n",
      "[2022-09-22 21:20:16.441 algo-1:27 INFO hook.py:591] name:pre_classifier.weight count_params:589824\n",
      "[2022-09-22 21:20:16.441 algo-1:27 INFO hook.py:591] name:pre_classifier.bias count_params:768\n",
      "[2022-09-22 21:20:16.441 algo-1:27 INFO hook.py:591] name:classifier.weight count_params:4608\n",
      "[2022-09-22 21:20:16.442 algo-1:27 INFO hook.py:591] name:classifier.bias count_params:6\n",
      "[2022-09-22 21:20:16.442 algo-1:27 INFO hook.py:593] Total Trainable Params: 66958086\n",
      "[2022-09-22 21:20:16.442 algo-1:27 INFO hook.py:425] Monitoring the collections: losses\n",
      "[2022-09-22 21:20:16.444 algo-1:27 INFO hook.py:488] Hook is writing from the hook with pid: 27\n",
      "{'loss': 0.9316, 'learning_rate': 2.994e-05, 'epoch': 1.0}\n",
      "2022-09-22 21:23:15,412 - /opt/conda/lib/python3.6/site-packages/datasets/metric.py - INFO - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "{'eval_loss': 0.26253288984298706, 'eval_accuracy': 0.9145, 'eval_runtime': 9.8667, 'eval_samples_per_second': 202.702, 'epoch': 1.0}\n",
      "{'train_runtime': 182.0405, 'train_samples_per_second': 2.747, 'epoch': 1.0}\n",
      "2022-09-22 21:23:27,892 - /opt/conda/lib/python3.6/site-packages/datasets/metric.py - INFO - Removing /root/.cache/huggingface/metrics/accuracy/default/default_experiment-1-0.arrow\n",
      "***** Eval results *****\n",
      "epoch = 1.0\n",
      "eval_accuracy = 0.9145\n",
      "eval_loss = 0.26253288984298706\n",
      "eval_mem_cpu_alloc_delta = 0\n",
      "eval_mem_cpu_peaked_delta = 0\n",
      "eval_mem_gpu_alloc_delta = 0\n",
      "eval_mem_gpu_peaked_delta = 2315885568\n",
      "eval_runtime = 9.89\n",
      "eval_samples_per_second = 202.224\n",
      "#015Downloading:   0%|          | 0.00/1.36k [00:00<?, ?B/s]#015Downloading: 2.92kB [00:00, 2.35MB/s]                   \n",
      "#015Downloading:   0%|          | 0.00/483 [00:00<?, ?B/s]#015Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 483/483 [00:00<00:00, 355kB/s]\n",
      "#015Downloading:   0%|          | 0.00/268M [00:00<?, ?B/s]#015Downloading:   1%|‚ñè         | 3.77M/268M [00:00<00:07, 37.7MB/s]#015Downloading:   3%|‚ñé         | 8.41M/268M [00:00<00:06, 39.9MB/s]#015Downloading:   5%|‚ñç         | 12.8M/268M [00:00<00:06, 41.0MB/s]#015Downloading:   7%|‚ñã         | 17.5M/268M [00:00<00:05, 42.7MB/s]#015Downloading:   8%|‚ñä         | 22.4M/268M [00:00<00:05, 44.4MB/s]#015Downloading:  10%|‚ñà         | 27.3M/268M [00:00<00:05, 45.5MB/s]#015Downloading:  12%|‚ñà‚ñè        | 32.2M/268M [00:00<00:05, 46.7MB/s]#015Downloading:  14%|‚ñà‚ñç        | 37.2M/268M [00:00<00:04, 47.5MB/s]#015Downloading:  16%|‚ñà‚ñå        | 42.1M/268M [00:00<00:04, 48.1MB/s]#015Downloading:  18%|‚ñà‚ñä        | 47.1M/268M [00:01<00:04, 48.5MB/s]#015Downloading:  19%|‚ñà‚ñâ        | 51.8M/268M [00:01<00:04, 47.5MB/s]#015Downloading:  21%|‚ñà‚ñà        | 56.5M/268M [00:01<00:04, 46.4MB/s]#015Downloading:  23%|‚ñà‚ñà‚ñé       | 61.1M/268M [00:01<00:04, 43.4MB/s]#015Downloading:  24%|‚ñà‚ñà‚ñç       | 65.6M/268M [00:01<00:04, 43.6MB/s]#015Downloading:  26%|‚ñà‚ñà‚ñå       | 69.9M/268M [00:01<00:04, 41.5MB/s]#015Downloading:  28%|‚ñà‚ñà‚ñä       | 74.3M/268M [00:01<00:04, 42.1MB/s]#015Downloading:  30%|‚ñà‚ñà‚ñâ       | 80.1M/268M [00:01<00:04, 45.8MB/s]#015Downloading:  32%|‚ñà‚ñà‚ñà‚ñè      | 85.9M/268M [00:01<00:03, 48.9MB/s]#015Downloading:  34%|‚ñà‚ñà‚ñà‚ñç      | 91.9M/268M [00:01<00:03, 51.7MB/s]#015Downloading:  37%|‚ñà‚ñà‚ñà‚ñã      | 97.8M/268M [00:02<00:03, 53.9MB/s]#015Downloading:  39%|‚ñà‚ñà‚ñà‚ñä      | 104M/268M [00:02<00:02, 55.2MB/s] #015Downloading:  41%|‚ñà‚ñà‚ñà‚ñà      | 109M/268M [00:02<00:03, 50.9MB/s]#015Downloading:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 115M/268M [00:02<00:03, 50.9MB/s]#015Downloading:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 120M/268M [00:02<00:02, 51.5MB/s]#015Downloading:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 125M/268M [00:02<00:02, 51.3MB/s]#015Downloading:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 130M/268M [00:02<00:02, 49.4MB/s]#015Downloading:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 136M/268M [00:02<00:02, 50.6MB/s]#015Downloading:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 141M/268M [00:02<00:02, 51.8MB/s]#015Downloading:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 146M/268M [00:03<00:02, 51.5MB/s]#015Downloading:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 151M/268M [00:03<00:02, 51.4MB/s]#015Downloading:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 157M/268M [00:03<00:02, 52.3MB/s]#015Downloading:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 162M/268M [00:03<00:01, 53.0MB/s]#015Downloading:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 168M/268M [00:03<00:01, 54.9MB/s]#015Downloading:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 174M/268M [00:03<00:01, 56.4MB/s]#015Downloading:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 180M/268M [00:03<00:01, 57.3MB/s]#015Downloading:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 186M/268M [00:03<00:01, 57.6MB/s]#015Downloading:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 192M/268M [00:03<00:01, 57.6MB/s]#015Downloading:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 198M/268M [00:03<00:01, 58.1MB/s]#015Downloading:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 204M/268M [00:04<00:01, 58.8MB/s]#015Downloading:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 210M/268M [00:04<00:00, 59.3MB/s]#015Downloading:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 216M/268M [00:04<00:00, 59.6MB/s]#015Downloading:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 222M/268M [00:04<00:00, 58.5MB/s]#015Downloading:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 228M/268M [00:04<00:00, 59.0MB/s]#015Downloading:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 234M/268M [00:04<00:00, 59.5MB/s]#015Downloading:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 240M/268M [00:04<00:00, 59.8MB/s]#015Downloading:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 246M/268M [00:04<00:00, 60.0MB/s]#015Downloading:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 252M/268M [00:04<00:00, 60.2MB/s]#015Downloading:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 258M/268M [00:04<00:00, 60.4MB/s]#015Downloading:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 264M/268M [00:05<00:00, 60.4MB/s]#015Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 268M/268M [00:05<00:00, 52.8MB/s]\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading:  35%|‚ñà‚ñà‚ñà‚ñç      | 80.9k/232k [00:00<00:00, 778kB/s]#015Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 232k/232k [00:00<00:00, 1.25MB/s]\n",
      "#015Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]#015Downloading:   7%|‚ñã         | 32.8k/466k [00:00<00:01, 258kB/s]#015Downloading:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 197k/466k [00:00<00:00, 344kB/s] #015Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 466k/466k [00:00<00:00, 1.45MB/s]\n",
      "#015Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]#015Downloading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.0/28.0 [00:00<00:00, 27.9kB/s]\n",
      "#015  0%|          | 0/500 [00:00<?, ?it/s]#015  0%|          | 1/500 [00:02<17:34,  2.11s/it]#015  0%|          | 2/500 [00:02<13:08,  1.58s/it]#015  1%|          | 3/500 [00:02<10:03,  1.22s/it]#015  1%|          | 4/500 [00:03<07:55,  1.04it/s]#015  1%|          | 5/500 [00:03<06:23,  1.29it/s]#015  1%|          | 6/500 [00:03<05:17,  1.56it/s]#015  1%|‚ñè         | 7/500 [00:04<04:35,  1.79it/s]#015  2%|‚ñè         | 8/500 [00:04<04:03,  2.02it/s]#015  2%|‚ñè         | 9/500 [00:04<03:36,  2.26it/s]#015  2%|‚ñè         | 10/500 [00:05<03:20,  2.45it/s]#015  2%|‚ñè         | 11/500 [00:05<03:06,  2.62it/s]#015  2%|‚ñè         | 12/500 [00:05<03:00,  2.70it/s]#015  3%|‚ñé         | 13/500 [00:06<02:53,  2.81it/s]#015  3%|‚ñé         | 14/500 [00:06<02:48,  2.89it/s]#015  3%|‚ñé         | 15/500 [00:06<02:44,  2.95it/s]#015  3%|‚ñé         | 16/500 [00:07<02:40,  3.01it/s]#015  3%|‚ñé         | 17/500 [00:07<02:41,  3.00it/s]#015  4%|‚ñé         | 18/500 [00:07<02:43,  2.95it/s]#015  4%|‚ñç         | 19/500 [00:08<02:39,  3.01it/s]#015  4%|‚ñç         | 20/500 [00:08<02:37,  3.06it/s]#015  4%|‚ñç         | 21/500 [00:08<02:35,  3.08it/s]#015  4%|‚ñç         | 22/500 [00:09<02:35,  3.07it/s]#015  5%|‚ñç         | 23/500 [00:09<02:35,  3.06it/s]#015  5%|‚ñç         | 24/500 [00:09<02:36,  3.05it/s]#015  5%|‚ñå         | 25/500 [00:10<02:37,  3.02it/s]#015  5%|‚ñå         | 26/500 [00:10<02:43,  2.89it/s]#015  5%|‚ñå         | 27/500 [00:10<02:41,  2.93it/s]#015  6%|‚ñå         | 28/500 [00:11<02:40,  2.95it/s]#015  6%|‚ñå         | 29/500 [00:11<02:37,  2.98it/s]#015  6%|‚ñå         | 30/500 [00:11<02:39,  2.94it/s]#015  6%|‚ñå         | 31/500 [00:12<02:36,  2.99it/s]#015  6%|‚ñã         | 32/500 [00:12<02:35,  3.02it/s]#015  7%|‚ñã         | 33/500 [00:12<02:38,  2.94it/s]#015  7%|‚ñã         | 34/500 [00:13<02:39,  2.93it/s]#015  7%|‚ñã         | 35/500 [00:13<02:36,  2.98it/s]#015  7%|‚ñã         | 36/500 [00:13<02:39,  2.90it/s]#015  7%|‚ñã         | 37/500 [00:14<02:35,  2.98it/s]#015  8%|‚ñä         | 38/500 [00:14<02:34,  2.99it/s]#015  8%|‚ñä         | 39/500 [00:14<02:34,  2.99it/s]#015  8%|‚ñä         | 40/500 [00:15<02:36,  2.94it/s]#015  8%|‚ñä         | 41/500 [00:15<02:35,  2.94it/s]#015  8%|‚ñä         | 42/500 [00:15<02:38,  2.90it/s]#015  9%|‚ñä         | 43/500 [00:16<02:40,  2.85it/s]#015  9%|‚ñâ         | 44/500 [00:16<02:38,  2.87it/s]#015  9%|‚ñâ         | 45/500 [00:16<02:38,  2.87it/s]#015  9%|‚ñâ         | 46/500 [00:17<02:34,  2.93it/s]#015  9%|‚ñâ         | 47/500 [00:17<02:33,  2.94it/s]#015 10%|‚ñâ         | 48/500 [00:17<02:30,  3.00it/s]#015 10%|‚ñâ         | 49/500 [00:18<02:29,  3.01it/s]#015 10%|‚ñà         | 50/500 [00:18<02:30,  2.99it/s]#015 10%|‚ñà         | 51/500 [00:18<02:28,  3.03it/s]#015 10%|‚ñà         | 52/500 [00:19<02:28,  3.01it/s]#015 11%|‚ñà         | 53/500 [00:19<02:28,  3.02it/s]#015 11%|‚ñà         | 54/500 [00:19<02:27,  3.03it/s]#015 11%|‚ñà         | 55/500 [00:20<02:26,  3.03it/s]#015 11%|‚ñà         | 56/500 [00:20<02:23,  3.09it/s]#015 11%|‚ñà‚ñè        | 57/500 [00:20<02:24,  3.07it/s]#015 12%|‚ñà‚ñè        | 58/500 [00:21<02:23,  3.07it/s]#015 12%|‚ñà‚ñè        | 59/500 [00:21<02:26,  3.01it/s]#015 12%|‚ñà‚ñè        | 60/500 [00:21<02:24,  3.04it/s]#015 12%|‚ñà‚ñè        | 61/500 [00:22<02:26,  3.01it/s]#015 12%|‚ñà‚ñè        | 62/500 [00:22<02:25,  3.02it/s]#015 13%|‚ñà‚ñé        | 63/500 [00:22<02:22,  3.07it/s]#015 13%|‚ñà‚ñé        | 64/500 [00:23<02:24,  3.02it/s]#015 13%|‚ñà‚ñé        | 65/500 [00:23<02:23,  3.02it/s]#015 13%|‚ñà‚ñé        | 66/500 [00:23<02:22,  3.04it/s]#015 13%|‚ñà‚ñé        | 67/500 [00:24<02:25,  2.97it/s]#015 14%|‚ñà‚ñé        | 68/500 [00:24<02:27,  2.94it/s]#015 14%|‚ñà‚ñç        | 69/500 [00:24<02:24,  2.98it/s]#015 14%|‚ñà‚ñç        | 70/500 [00:25<02:22,  3.02it/s]#015 14%|‚ñà‚ñç        | 71/500 [00:25<02:22,  3.01it/s]#015 14%|‚ñà‚ñç        | 72/500 [00:25<02:23,  2.98it/s]#015 15%|‚ñà‚ñç        | 73/500 [00:26<02:23,  2.98it/s]#015 15%|‚ñà‚ñç        | 74/500 [00:26<02:24,  2.96it/s]#015 15%|‚ñà‚ñå        | 75/500 [00:26<02:21,  3.00it/s]#015 15%|‚ñà‚ñå        | 76/500 [00:27<02:20,  3.02it/s]#015 15%|‚ñà‚ñå        | 77/500 [00:27<02:21,  2.99it/s]#015 16%|‚ñà‚ñå        | 78/500 [00:27<02:21,  2.98it/s]#015 16%|‚ñà‚ñå        | 79/500 [00:28<02:19,  3.01it/s]#015 16%|‚ñà‚ñå        | 80/500 [00:28<02:20,  3.00it/s]#015 16%|‚ñà‚ñå        | 81/500 [00:28<02:20,  2.98it/s]#015 16%|‚ñà‚ñã        | 82/500 [00:29<02:18,  3.01it/s]#015 17%|‚ñà‚ñã        | 83/500 [00:29<02:19,  2.98it/s]#015 17%|‚ñà‚ñã        | 84/500 [00:29<02:18,  3.01it/s]#015 17%|‚ñà‚ñã        | 85/500 [00:30<02:18,  3.01it/s]#015 17%|‚ñà‚ñã        | 86/500 [00:30<02:16,  3.03it/s]#015 17%|‚ñà‚ñã        | 87/500 [00:30<02:15,  3.04it/s]#015 18%|‚ñà‚ñä        | 88/500 [00:31<02:16,  3.02it/s]#015 18%|‚ñà‚ñä        | 89/500 [00:31<02:18,  2.97it/s]#015 18%|‚ñà‚ñä        | 90/500 [00:31<02:17,  2.97it/s]#015 18%|‚ñà‚ñä        | 91/500 [00:32<02:17,  2.97it/s]#015 18%|‚ñà‚ñä        | 92/500 [00:32<02:14,  3.03it/s]#015 19%|‚ñà‚ñä        | 93/500 [00:32<02:15,  3.01it/s]#015 19%|‚ñà‚ñâ        | 94/500 [00:33<02:13,  3.03it/s]#015 19%|‚ñà‚ñâ        | 95/500 [00:33<02:13,  3.03it/s]#015 19%|‚ñà‚ñâ        | 96/500 [00:33<02:12,  3.04it/s]#015 19%|‚ñà‚ñâ        | 97/500 [00:34<02:12,  3.05it/s]#015 20%|‚ñà‚ñâ        | 98/500 [00:34<02:11,  3.05it/s]#015 20%|‚ñà‚ñâ        | 99/500 [00:34<02:10,  3.06it/s]#015 20%|‚ñà‚ñà        | 100/500 [00:35<02:10,  3.07it/s]#015 20%|‚ñà‚ñà        | 101/500 [00:35<02:11,  3.02it/s]#015 20%|‚ñà‚ñà        | 102/500 [00:35<02:12,  3.00it/s]#015 21%|‚ñà‚ñà        | 103/500 [00:36<02:12,  3.00it/s]#015 21%|‚ñà‚ñà        | 104/500 [00:36<02:11,  3.01it/s]#015 21%|‚ñà‚ñà        | 105/500 [00:36<02:12,  2.97it/s]#015 21%|‚ñà‚ñà        | 106/500 [00:37<02:10,  3.01it/s]#015 21%|‚ñà‚ñà‚ñè       | 107/500 [00:37<02:08,  3.06it/s]#015 22%|‚ñà‚ñà‚ñè       | 108/500 [00:37<02:08,  3.05it/s]#015 22%|‚ñà‚ñà‚ñè       | 109/500 [00:38<02:06,  3.09it/s]#015 22%|‚ñà‚ñà‚ñè       | 110/500 [00:38<02:08,  3.04it/s]#015 22%|‚ñà‚ñà‚ñè       | 111/500 [00:38<02:13,  2.92it/s]#015 22%|‚ñà‚ñà‚ñè       | 112/500 [00:39<02:09,  3.00it/s]#015 23%|‚ñà‚ñà‚ñé       | 113/500 [00:39<02:11,  2.94it/s]#015 23%|‚ñà‚ñà‚ñé       | 114/500 [00:39<02:08,  3.00it/s]#015 23%|‚ñà‚ñà‚ñé       | 115/500 [00:40<02:06,  3.05it/s]#015 23%|‚ñà‚ñà‚ñé       | 116/500 [00:40<02:04,  3.09it/s]#015 23%|‚ñà‚ñà‚ñé       | 117/500 [00:40<02:07,  3.00it/s]#015 24%|‚ñà‚ñà‚ñé       | 118/500 [00:41<02:07,  3.00it/s]#015 24%|‚ñà‚ñà‚ñç       | 119/500 [00:41<02:06,  3.02it/s]#015 24%|‚ñà‚ñà‚ñç       | 120/500 [00:41<02:09,  2.94it/s]#015 24%|‚ñà‚ñà‚ñç       | 121/500 [00:42<02:06,  3.00it/s]#015 24%|‚ñà‚ñà‚ñç       | 122/500 [00:42<02:08,  2.94it/s]#015 25%|‚ñà‚ñà‚ñç       | 123/500 [00:42<02:06,  2.97it/s]#015 25%|‚ñà‚ñà‚ñç       | 124/500 [00:43<02:07,  2.96it/s]#015 25%|‚ñà‚ñà‚ñå       | 125/500 [00:43<02:06,  2.97it/s]#015 25%|‚ñà‚ñà‚ñå       | 126/500 [00:43<02:05,  2.98it/s]#015 25%|‚ñà‚ñà‚ñå       | 127/500 [00:44<02:03,  3.01it/s]#015 26%|‚ñà‚ñà‚ñå       | 128/500 [00:44<02:05,  2.98it/s]#015 26%|‚ñà‚ñà‚ñå       | 129/500 [00:44<02:05,  2.95it/s]#015 26%|‚ñà‚ñà‚ñå       | 130/500 [00:45<02:06,  2.92it/s]#015 26%|‚ñà‚ñà‚ñå       | 131/500 [00:45<02:03,  2.99it/s]#015 26%|‚ñà‚ñà‚ñã       | 132/500 [00:45<02:02,  3.00it/s]#015 27%|‚ñà‚ñà‚ñã       | 133/500 [00:46<02:03,  2.97it/s]#015 27%|‚ñà‚ñà‚ñã       | 134/500 [00:46<02:05,  2.92it/s]#015 27%|‚ñà‚ñà‚ñã       | 135/500 [00:46<02:03,  2.96it/s]#015 27%|‚ñà‚ñà‚ñã       | 136/500 [00:47<02:01,  3.01it/s]#015 27%|‚ñà‚ñà‚ñã       | 137/500 [00:47<01:59,  3.03it/s]#015 28%|‚ñà‚ñà‚ñä       | 138/500 [00:47<02:02,  2.96it/s]#015 28%|‚ñà‚ñà‚ñä       | 139/500 [00:48<02:03,  2.93it/s]#015 28%|‚ñà‚ñà‚ñä       | 140/500 [00:48<02:02,  2.95it/s]#015 28%|‚ñà‚ñà‚ñä       | 141/500 [00:48<02:02,  2.92it/s]#015 28%|‚ñà‚ñà‚ñä       | 142/500 [00:49<02:03,  2.89it/s]#015 29%|‚ñà‚ñà‚ñä       | 143/500 [00:49<02:06,  2.83it/s]#015 29%|‚ñà‚ñà‚ñâ       | 144/500 [00:50<02:06,  2.81it/s]#015 29%|‚ñà‚ñà‚ñâ       | 145/500 [00:50<02:05,  2.83it/s]#015 29%|‚ñà‚ñà‚ñâ       | 146/500 [00:50<02:03,  2.87it/s]#015 29%|‚ñà‚ñà‚ñâ       | 147/500 [00:51<02:02,  2.88it/s]#015 30%|‚ñà‚ñà‚ñâ       | 148/500 [00:51<02:02,  2.88it/s]#015 30%|‚ñà‚ñà‚ñâ       | 149/500 [00:51<02:02,  2.86it/s]#015 30%|‚ñà‚ñà‚ñà       | 150/500 [00:52<02:04,  2.82it/s]#015 30%|‚ñà‚ñà‚ñà       | 151/500 [00:52<02:02,  2.86it/s]#015 30%|‚ñà‚ñà‚ñà       | 152/500 [00:52<01:58,  2.93it/s]#015 31%|‚ñà‚ñà‚ñà       | 153/500 [00:53<01:56,  2.97it/s]#015 31%|‚ñà‚ñà‚ñà       | 154/500 [00:53<01:54,  3.02it/s]#015 31%|‚ñà‚ñà‚ñà       | 155/500 [00:53<01:54,  3.01it/s]#015 31%|‚ñà‚ñà‚ñà       | 156/500 [00:54<01:56,  2.96it/s]#015 31%|‚ñà‚ñà‚ñà‚ñè      | 157/500 [00:54<01:55,  2.96it/s]#015 32%|‚ñà‚ñà‚ñà‚ñè      | 158/500 [00:54<01:56,  2.94it/s]#015 32%|‚ñà‚ñà‚ñà‚ñè      | 159/500 [00:55<01:53,  3.01it/s]#015 32%|‚ñà‚ñà‚ñà‚ñè      | 160/500 [00:55<01:52,  3.01it/s]#015 32%|‚ñà‚ñà‚ñà‚ñè      | 161/500 [00:55<01:52,  3.01it/s]#015 32%|‚ñà‚ñà‚ñà‚ñè      | 162/500 [00:56<01:50,  3.06it/s]#015 33%|‚ñà‚ñà‚ñà‚ñé      | 163/500 [00:56<01:50,  3.06it/s]#015 33%|‚ñà‚ñà‚ñà‚ñé      | 164/500 [00:56<01:49,  3.07it/s]#015 33%|‚ñà‚ñà‚ñà‚ñé      | 165/500 [00:57<01:51,  3.00it/s]#015 33%|‚ñà‚ñà‚ñà‚ñé      | 166/500 [00:57<01:53,  2.94it/s]#015 33%|‚ñà‚ñà‚ñà‚ñé      | 167/500 [00:57<01:52,  2.97it/s]#015 34%|‚ñà‚ñà‚ñà‚ñé      | 168/500 [00:58<01:49,  3.02it/s]#015 34%|‚ñà‚ñà‚ñà‚ñç      | 169/500 [00:58<01:48,  3.06it/s]#015 34%|‚ñà‚ñà‚ñà‚ñç      | 170/500 [00:58<01:47,  3.08it/s]#015 34%|‚ñà‚ñà‚ñà‚ñç      | 171/500 [00:59<01:46,  3.08it/s]#015 34%|‚ñà‚ñà‚ñà‚ñç      | 172/500 [00:59<01:46,  3.09it/s]#015 35%|‚ñà‚ñà‚ñà‚ñç      | 173/500 [00:59<01:48,  3.03it/s]#015 35%|‚ñà‚ñà‚ñà‚ñç      | 174/500 [01:00<01:47,  3.03it/s]#015 35%|‚ñà‚ñà‚ñà‚ñå      | 175/500 [01:00<01:46,  3.05it/s]#015 35%|‚ñà‚ñà‚ñà‚ñå      | 176/500 [01:00<01:48,  3.00it/s]#015 35%|‚ñà‚ñà‚ñà‚ñå      | 177/500 [01:01<01:46,  3.02it/s]#015 36%|‚ñà‚ñà‚ñà‚ñå      | 178/500 [01:01<01:46,  3.02it/s]#015 36%|‚ñà‚ñà‚ñà‚ñå      | 179/500 [01:01<01:39,  3.24it/s]#015 36%|‚ñà‚ñà‚ñà‚ñå      | 180/500 [01:01<01:40,  3.20it/s]#015 36%|‚ñà‚ñà‚ñà‚ñå      | 181/500 [01:02<01:42,  3.12it/s]#015 36%|‚ñà‚ñà‚ñà‚ñã      | 182/500 [01:02<01:43,  3.09it/s]#015 37%|‚ñà‚ñà‚ñà‚ñã      | 183/500 [01:03<01:48,  2.93it/s]#015 37%|‚ñà‚ñà‚ñà‚ñã      | 184/500 [01:03<01:46,  2.97it/s]#015 37%|‚ñà‚ñà‚ñà‚ñã      | 185/500 [01:03<01:46,  2.96it/s]#015 37%|‚ñà‚ñà‚ñà‚ñã      | 186/500 [01:04<01:45,  2.96it/s]#015 37%|‚ñà‚ñà‚ñà‚ñã      | 187/500 [01:04<01:48,  2.90it/s]#015 38%|‚ñà‚ñà‚ñà‚ñä      | 188/500 [01:04<01:47,  2.90it/s]#015 38%|‚ñà‚ñà‚ñà‚ñä      | 189/500 [01:05<01:46,  2.93it/s]#015 38%|‚ñà‚ñà‚ñà‚ñä      | 190/500 [01:05<01:44,  2.96it/s]#015 38%|‚ñà‚ñà‚ñà‚ñä      | 191/500 [01:05<01:43,  2.98it/s]#015 38%|‚ñà‚ñà‚ñà‚ñä      | 192/500 [01:06<01:42,  3.00it/s]#015 39%|‚ñà‚ñà‚ñà‚ñä      | 193/500 [01:06<01:42,  3.01it/s]#015 39%|‚ñà‚ñà‚ñà‚ñâ      | 194/500 [01:06<01:44,  2.92it/s]#015 39%|‚ñà‚ñà‚ñà‚ñâ      | 195/500 [01:07<01:43,  2.96it/s]#015 39%|‚ñà‚ñà‚ñà‚ñâ      | 196/500 [01:07<01:41,  3.00it/s]#015 39%|‚ñà‚ñà‚ñà‚ñâ      | 197/500 [01:07<01:41,  2.99it/s]#015 40%|‚ñà‚ñà‚ñà‚ñâ      | 198/500 [01:08<01:42,  2.96it/s]#015 40%|‚ñà‚ñà‚ñà‚ñâ      | 199/500 [01:08<01:40,  3.01it/s]#015 40%|‚ñà‚ñà‚ñà‚ñà      | 200/500 [01:08<01:38,  3.04it/s]#015 40%|‚ñà‚ñà‚ñà‚ñà      | 201/500 [01:09<01:41,  2.94it/s]#015 40%|‚ñà‚ñà‚ñà‚ñà      | 202/500 [01:09<01:41,  2.94it/s]#015 41%|‚ñà‚ñà‚ñà‚ñà      | 203/500 [01:09<01:41,  2.93it/s]#015 41%|‚ñà‚ñà‚ñà‚ñà      | 204/500 [01:10<01:39,  2.97it/s]#015 41%|‚ñà‚ñà‚ñà‚ñà      | 205/500 [01:10<01:38,  2.99it/s]#015 41%|‚ñà‚ñà‚ñà‚ñà      | 206/500 [01:10<01:37,  3.01it/s]#015 41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 207/500 [01:11<01:37,  3.02it/s]#015 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 208/500 [01:11<01:36,  3.04it/s]#015 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 209/500 [01:11<01:36,  3.01it/s]#015 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 210/500 [01:12<01:35,  3.03it/s]#015 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 211/500 [01:12<01:34,  3.07it/s]#015 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 212/500 [01:12<01:35,  3.02it/s]#015 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 213/500 [01:13<01:36,  2.99it/s]#015 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 214/500 [01:13<01:34,  3.02it/s]#015 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 215/500 [01:13<01:36,  2.94it/s]#015 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 216/500 [01:14<01:35,  2.98it/s]#015 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 217/500 [01:14<01:33,  3.03it/s]#015 44%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 218/500 [01:14<01:32,  3.05it/s]#015 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 219/500 [01:15<01:33,  3.02it/s]#015 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 220/500 [01:15<01:32,  3.03it/s]#015 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 221/500 [01:15<01:32,  3.01it/s]#015 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 222/500 [01:16<01:31,  3.04it/s]#015 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 223/500 [01:16<01:31,  3.03it/s]#015 45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 224/500 [01:16<01:32,  2.99it/s]#015 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 225/500 [01:17<01:34,  2.91it/s]#015 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 226/500 [01:17<01:33,  2.93it/s]#015 45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 227/500 [01:17<01:31,  2.97it/s]#015 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 228/500 [01:18<01:30,  2.99it/s]#015 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 229/500 [01:18<01:30,  3.00it/s]#015 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 230/500 [01:18<01:30,  2.99it/s]#015 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 231/500 [01:19<01:29,  3.02it/s]#015 46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 232/500 [01:19<01:29,  3.01it/s]#015 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 233/500 [01:19<01:29,  2.99it/s]#015 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 234/500 [01:20<01:29,  2.99it/s]#015 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 235/500 [01:20<01:28,  3.01it/s]#015 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 236/500 [01:20<01:27,  3.01it/s]#015 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 237/500 [01:21<01:26,  3.04it/s]#015 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 238/500 [01:21<01:26,  3.03it/s]#015 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 239/500 [01:21<01:24,  3.08it/s]#015 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 240/500 [01:22<01:24,  3.08it/s]#015 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 241/500 [01:22<01:23,  3.11it/s]#015 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 242/500 [01:22<01:24,  3.07it/s]#015 49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 243/500 [01:23<01:25,  3.00it/s]#015 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 244/500 [01:23<01:25,  2.99it/s]#015 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 245/500 [01:23<01:25,  3.00it/s]#015 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 246/500 [01:24<01:24,  3.01it/s]#015 49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 247/500 [01:24<01:24,  3.00it/s]#015 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 248/500 [01:24<01:25,  2.95it/s]#015 50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 249/500 [01:25<01:23,  3.00it/s]#015 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 250/500 [01:25<01:22,  3.04it/s]#015 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 251/500 [01:25<01:21,  3.07it/s]#015 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 252/500 [01:25<01:19,  3.10it/s]#015 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 253/500 [01:26<01:18,  3.14it/s]#015 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 254/500 [01:26<01:18,  3.15it/s]#015 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 255/500 [01:26<01:17,  3.18it/s]#015 51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 256/500 [01:27<01:18,  3.10it/s]#015 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 257/500 [01:27<01:18,  3.09it/s]#015 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 258/500 [01:27<01:18,  3.06it/s]#015 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 259/500 [01:28<01:18,  3.07it/s]#015 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 260/500 [01:28<01:18,  3.05it/s]#015 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 261/500 [01:29<01:39,  2.39it/s]#015 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 262/500 [01:29<01:52,  2.12it/s]#015 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 263/500 [01:30<02:04,  1.91it/s]#015 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 264/500 [01:30<01:58,  1.99it/s]#015 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 265/500 [01:31<01:45,  2.22it/s]#015 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 266/500 [01:31<01:36,  2.43it/s]#015 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 267/500 [01:31<01:30,  2.59it/s]#015 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 268/500 [01:32<01:24,  2.73it/s]#015 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 269/500 [01:32<01:22,  2.80it/s]#015 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 270/500 [01:32<01:20,  2.87it/s]#015 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 271/500 [01:33<01:18,  2.91it/s]#015 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 272/500 [01:33<01:17,  2.93it/s]#015 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 273/500 [01:33<01:16,  2.97it/s]#015 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 274/500 [01:34<01:15,  2.99it/s]#015 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 275/500 [01:34<01:14,  3.01it/s]#015 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 276/500 [01:34<01:13,  3.07it/s]#015 55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 277/500 [01:35<01:12,  3.07it/s]#015 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 278/500 [01:35<01:11,  3.09it/s]#015 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 279/500 [01:35<01:12,  3.06it/s]#015 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 280/500 [01:36<01:12,  3.04it/s]#015 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 281/500 [01:36<01:11,  3.06it/s]#015 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 282/500 [01:36<01:11,  3.06it/s]#015 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 283/500 [01:37<01:10,  3.07it/s]#015 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 284/500 [01:37<01:10,  3.05it/s]#015 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 285/500 [01:37<01:10,  3.04it/s]#015 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 286/500 [01:38<01:10,  3.03it/s]#015 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 287/500 [01:38<01:09,  3.06it/s]#015 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 288/500 [01:38<01:09,  3.03it/s]#015 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 289/500 [01:39<01:09,  3.04it/s]#015 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 290/500 [01:39<01:08,  3.05it/s]#015 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 291/500 [01:39\n",
      "<01:08,  3.05it/s]#015 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 292/500 [01:40<01:08,  3.05it/s]#015 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 293/500 [01:40<01:08,  3.04it/s]#015 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 294/500 [01:40<01:07,  3.03it/s]#015 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 295/500 [01:41<01:08,  3.01it/s]#015 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 296/500 [01:41<01:08,  2.99it/s]#015 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 297/500 [01:41<01:07,  2.99it/s]#015 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 298/500 [01:42<01:07,  3.01it/s]#015 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 299/500 [01:42<01:06,  3.01it/s]#015 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 300/500 [01:42<01:07,  2.98it/s]#015 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 301/500 [01:43<01:06,  3.00it/s]#015 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 302/500 [01:43<01:06,  3.00it/s]#015 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 303/500 [01:43<01:06,  2.97it/s]#015 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 304/500 [01:44<01:05,  3.01it/s]#015 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 305/500 [01:44<01:06,  2.92it/s]#015 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 306/500 [01:44<01:07,  2.88it/s]#015 61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 307/500 [01:45<01:05,  2.94it/s]#015 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 308/500 [01:45<01:06,  2.88it/s]#015 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 309/500 [01:45<01:05,  2.92it/s]#015 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 310/500 [01:46<01:04,  2.96it/s]#015 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 311/500 [01:46<01:06,  2.86it/s]#015 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 312/500 [01:46<01:05,  2.86it/s]#015 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 313/500 [01:47<01:05,  2.87it/s]#015 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 314/500 [01:47<01:04,  2.88it/s]#015 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 315/500 [01:47<01:03,  2.93it/s]#015 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 316/500 [01:48<01:03,  2.91it/s]#015 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 317/500 [01:48<01:03,  2.89it/s]#015 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 318/500 [01:48<01:01,  2.96it/s]#015 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 319/500 [01:49<01:01,  2.92it/s]#015 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 320/500 [01:49<01:03,  2.82it/s]#015 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 321/500 [01:49<01:03,  2.84it/s]#015 64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 322/500 [01:50<01:03,  2.79it/s]#015 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 323/500 [01:50<01:03,  2.77it/s]#015 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 324/500 [01:51<01:01,  2.85it/s]#015 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 325/500 [01:51<01:00,  2.89it/s]#015 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 326/500 [01:51<01:00,  2.90it/s]#015 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 327/500 [01:52<00:58,  2.94it/s]#015 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 328/500 [01:52<00:57,  3.00it/s]#015 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 329/500 [01:52<00:57,  2.99it/s]#015 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 330/500 [01:53<00:57,  2.93it/s]#015 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 331/500 [01:53<00:56,  2.99it/s]#015 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 332/500 [01:53<00:56,  2.98it/s]#015 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 333/500 [01:54<00:55,  3.02it/s]#015 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 334/500 [01:54<00:53,  3.08it/s]#015 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 335/500 [01:54<00:53,  3.10it/s]#015 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 336/500 [01:54<00:52,  3.11it/s]#015 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 337/500 [01:55<00:53,  3.07it/s]#015 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 338/500 [01:55<00:53,  3.04it/s]#015 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 339/500 [01:55<00:52,  3.05it/s]#015 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 340/500 [01:56<00:53,  3.01it/s]#015 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 341/500 [01:56<00:51,  3.08it/s]#015 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 342/500 [01:56<00:52,  3.01it/s]#015 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 343/500 [01:57<00:52,  2.99it/s]#015 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 344/500 [01:57<00:52,  2.99it/s]#015 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 345/500 [01:57<00:51,  3.02it/s]#015 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 346/500 [01:58<00:50,  3.03it/s]#015 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 347/500 [01:58<00:51,  2.96it/s]#015 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 348/500 [01:58<00:50,  3.02it/s]#015 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 349/500 [01:59<00:49,  3.05it/s]#015 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 350/500 [01:59<00:49,  3.06it/s]#015 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 351/500 [01:59<00:48,  3.05it/s]#015 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 352/500 [02:00<00:48,  3.07it/s]#015 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 353/500 [02:00<00:48,  3.04it/s]#015 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 354/500 [02:00<00:47,  3.06it/s]#015 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 355/500 [02:01<00:48,  3.01it/s]#015 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 356/500 [02:01<00:48,  2.97it/s]#015 71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 357/500 [02:01<00:48,  2.93it/s]#015 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 358/500 [02:02<00:47,  2.99it/s]#015 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 359/500 [02:02<00:45,  3.07it/s]#015 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 360/500 [02:02<00:45,  3.06it/s]#015 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 361/500 [02:03<00:45,  3.07it/s]#015 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 362/500 [02:03<00:45,  3.05it/s]#015 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 363/500 [02:03<00:45,  2.98it/s]#015 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 364/500 [02:04<00:45,  3.00it/s]#015 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 365/500 [02:04<00:45,  2.97it/s]#015 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 366/500 [02:04<00:44,  3.00it/s]#015 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 367/500 [02:05<00:44,  2.99it/s]#015 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 368/500 [02:05<00:43,  3.00it/s]#015 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 369/500 [02:05<00:43,  2.98it/s]#015 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 370/500 [02:06<00:43,  2.97it/s]#015 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 371/500 [02:06<00:42,  3.05it/s]#015 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 372/500 [02:06<00:41,  3.10it/s]#015 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 373/500 [02:07<00:41,  3.10it/s]#015 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 374/500 [02:07<00:40,  3.10it/s]#015 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 375/500 [02:07<00:41,  3.04it/s]#015 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 376/500 [02:08<00:41,  3.02it/s]#015 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 377/500 [02:08<00:41,  2.98it/s]#015 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 378/500 [02:08<00:40,  3.01it/s]#015 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 379/500 [02:09<00:39,  3.03it/s]#015 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 380/500 [02:09<00:39,  3.05it/s]#015 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 381/500 [02:09<00:39,  3.04it/s]#015 76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 382/500 [02:10<00:38,  3.04it/s]#015 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 383/500 [02:10<00:38,  3.03it/s]#015 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 384/500 [02:10<00:38,  2.99it/s]#015 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 385/500 [02:11<00:38,  2.97it/s]#015 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 386/500 [02:11<00:38,  2.93it/s]#015 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 387/500 [02:11<00:38,  2.96it/s]#015 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 388/500 [02:12<00:37,  3.00it/s]#015 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 389/500 [02:12<00:37,  2.92it/s]#015 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 390/500 [02:12<00:36,  2.99it/s]#015 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 391/500 [02:13<00:36,  2.97it/s]#015 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 392/500 [02:13<00:36,  2.99it/s]#015 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 393/500 [02:13<00:35,  2.98it/s]#015 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 394/500 [02:14<00:35,  2.98it/s]#015 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 395/500 [02:14<00:35,  2.97it/s]#015 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 396/500 [02:14<00:35,  2.95it/s]#015 79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 397/500 [02:15<00:34,  2.97it/s]#015 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 398/500 [02:15<00:34,  2.98it/s]#015 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 399/500 [02:15<00:34,  2.91it/s]#015 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 400/500 [02:16<00:33,  2.95it/s]#015 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 401/500 [02:16<00:33,  2.95it/s]#015 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 402/500 [02:16<00:33,  2.96it/s]#015 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 403/500 [02:17<00:33,  2.91it/s]#015 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 404/500 [02:17<00:32,  2.94it/s]#015 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 405/500 [02:17<00:31,  2.98it/s]#015 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 406/500 [02:18<00:31,  2.97it/s]#015 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 407/500 [02:18<00:30,  3.01it/s]#015 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 408/500 [02:18<00:30,  3.05it/s]#015 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 409/500 [02:19<00:30,  2.98it/s]#015 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 410/500 [02:19<00:29,  3.01it/s]#015 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 411/500 [02:19<00:29,  2.98it/s]#015 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 412/500 [02:20<00:30,  2.91it/s]#015 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 413/500 [02:20<00:29,  2.95it/s]#015 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 414/500 [02:21<00:29,  2.96it/s]#015 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 415/500 [02:21<00:28,  2.97it/s]#015 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 416/500 [02:21<00:27,  3.05it/s]#015 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 417/500 [02:21<00:27,  3.04it/s]#015 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 418/500 [02:22<00:27,  3.04it/s]#015 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 419/500 [02:22<00:26,  3.05it/s]#015 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 420/500 [02:22<00:26,  3.01it/s]#015 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 421/500 [02:23<00:26,  2.98it/s]#015 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 422/500 [02:23<00:26,  2.97it/s]#015 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 423/500 [02:24<00:26,  2.91it/s]#015 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 424/500 [02:24<00:25,  2.96it/s]#015 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 425/500 [02:24<00:25,  2.98it/s]#015 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 426/500 [02:25<00:24,  3.02it/s]#015 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 427/500 [02:25<00:24,  3.04it/s]#015 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 428/500 [02:25<00:23,  3.07it/s]#015 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 429/500 [02:25<00:23,  3.01it/s]#015 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 430/500 [02:26<00:23,  2.96it/s]#015 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 431/500 [02:26<00:23,  2.91it/s]#015 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 432/500 [02:27<00:23,  2.95it/s]#015 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 433/500 [02:27<00:22,  2.95it/s]#015 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 434/500 [02:27<00:22,  2.95it/s]#015 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 435/500 [02:28<00:21,  2.97it/s]#015 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 436/500 [02:28<00:21,  3.00it/s]#015 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 437/500 [02:28<00:21,  2.98it/s]#015 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 438/500 [02:29<00:20,  2.99it/s]#015 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 439/500 [02:29<00:20,  3.03it/s]#015 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 440/500 [02:29<00:19,  3.06it/s]#015 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 441/500 [02:30<00:19,  3.04it/s]#015 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 442/500 [02:30<00:18,  3.06it/s]#015 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 443/500 [02:30<00:19,  2.98it/s]#015 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 444/500 [02:31<00:18,  3.00it/s]#015 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 445/500 [02:31<00:18,  2.97it/s]#015 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 446/500 [02:31<00:18,  2.88it/s]#015 89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 447/500 [02:32<00:18,  2.84it/s]#015 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 448/500 [02:32<00:17,  2.93it/s]#015 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 449/500 [02:32<00:17,  2.99it/s]#015 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 450/500 [02:33<00:16,  3.03it/s]#015 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 451/500 [02:33<00:16,  2.99it/s]#015 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 452/500 [02:33<00:16,  2.95it/s]#015 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 453/500 [02:34<00:15,  2.98it/s]#015 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 454/500 [02:34<00:15,  2.98it/s]#015 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 455/500 [02:34<00:14,  3.04it/s]#015 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 456/500 [02:35<00:14,  3.07it/s]#015 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 457/500 [02:35<00:14,  3.06it/s]#015 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 458/500 [02:35<00:13,  3.04it/s]#015 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 459/500 [02:36<00:13,  2.96it/s]#015 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 460/500 [02:36<00:13,  3.01it/s]#015 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 461/500 [02:36<00:13,  2.99it/s]#015 92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 462/500 [02:37<00:12,  3.04it/s]#015 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 463/500 [02:37<00:12,  3.03it/s]#015 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 464/500 [02:37<00:11,  3.06it/s]#015 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 465/500 [02:38<00:11,  3.01it/s]#015 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 466/500 [02:38<00:11,  3.05it/s]#015 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 467/500 [02:38<00:10,  3.04it/s]#015 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 468/500 [02:39<00:10,  3.04it/s]#015 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 469/500 [02:39<00:10,  3.02it/s]#015 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 470/500 [02:39<00:09,  3.02it/s]#015 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 471/500 [02:39<00:09,  3.04it/s]#015 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 472/500 [02:40<00:09,  3.07it/s]#015 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 473/500 [02:40<00:08,  3.05it/s]#015 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 474/500 [02:40<00:08,  3.05it/s]#015 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 475/500 [02:41<00:08,  3.03it/s]#015 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 476/500 [02:41<00:07,  3.00it/s]#015 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 477/500 [02:42<00:07,  2.94it/s]#015 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 478/500 [02:42<00:07,  2.91it/s]#015 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 479/500 [02:42<00:07,  2.95it/s]#015 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 480/500 [02:43<00:06,  2.98it/s]#015 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 481/500 [02:43<00:06,  2.98it/s]#015 96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 482/500 [02:43<00:06,  2.99it/s]#015 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 483/500 [02:44<00:05,  2.95it/s]#015 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 484/500 [02:44<00:05,  2.98it/s]#015 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 485/500 [02:44<00:05,  2.95it/s]#015 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 486/500 [02:45<00:04,  2.93it/s]#015 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 487/500 [02:45<00:04,  2.91it/s]#015 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 488/500 [02:45<00:04,  2.94it/s]#015 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 489/500 [02:46<00:03,  3.01it/s]#015 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 490/500 [02:46<00:03,  3.00it/s]#015 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 491/500 [02:46<00:02,  3.04it/s]#015 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 492/500 [02:47<00:02,  3.04it/s]#015 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 493/500 [02:47<00:02,  3.05it/s]#015 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 494/500 [02:47<00:01,  3.02it/s]#015 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 495/500 [02:48<00:01,  3.05it/s]#015 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 496/500 [02:48<00:01,  2.97it/s]#015 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 497/500 [02:48<00:01,  2.95it/s]#015100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 498/500 [02:49<00:00,  2.97it/s]#015100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 499/500 [02:49<00:00,  2.93it/s]#015100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [02:49<00:00,  2.95it/s]#015                                                 #015#015100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [02:49<00:00,  2.95it/s]\n",
      "2022-09-22 21:23:29,453 sagemaker-training-toolkit INFO     Reporting training SUCCESS\n",
      "#015  0%|          | 0/32 [00:00<?, ?it/s]#033[A\n",
      "#015  6%|‚ñã         | 2/32 [00:00<00:04,  6.02it/s]#033[A\n",
      "#015  9%|‚ñâ         | 3/32 [00:00<00:06,  4.77it/s]#033[A\n",
      "#015 12%|‚ñà‚ñé        | 4/32 [00:00<00:06,  4.15it/s]#033[A\n",
      "#015 16%|‚ñà‚ñå        | 5/32 [00:01<00:07,  3.81it/s]#033[A\n",
      "#015 19%|‚ñà‚ñâ        | 6/32 [00:01<00:07,  3.60it/s]#033[A\n",
      "#015 22%|‚ñà‚ñà‚ñè       | 7/32 [00:01<00:07,  3.48it/s]#033[A\n",
      "#015 25%|‚ñà‚ñà‚ñå       | 8/32 [00:02<00:07,  3.39it/s]#033[A\n",
      "#015 28%|‚ñà‚ñà‚ñä       | 9/32 [00:02<00:06,  3.33it/s]#033[A\n",
      "#015 31%|‚ñà‚ñà‚ñà‚ñè      | 10/32 [00:02<00:06,  3.29it/s]#033[A\n",
      "#015 34%|‚ñà‚ñà‚ñà‚ñç      | 11/32 [00:03<00:06,  3.26it/s]#033[A\n",
      "#015 38%|‚ñà‚ñà‚ñà‚ñä      | 12/32 [00:03<00:06,  3.24it/s]#033[A\n",
      "#015 41%|‚ñà‚ñà‚ñà‚ñà      | 13/32 [00:03<00:05,  3.23it/s]#033[A\n",
      "#015 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 14/32 [00:04<00:05,  3.20it/s]#033[A\n",
      "#015 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 15/32 [00:04<00:05,  3.20it/s]#033[A\n",
      "#015 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 16/32 [00:04<00:04,  3.20it/s]#033[A\n",
      "#015 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 17/32 [00:05<00:04,  3.20it/s]#033[A\n",
      "#015 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 18/32 [00:05<00:04,  3.20it/s]#033[A\n",
      "#015 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 19/32 [00:05<00:04,  3.20it/s]#033[A\n",
      "#015 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 20/32 [00:05<00:03,  3.20it/s]#033[A\n",
      "#015 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 21/32 [00:06<00:03,  3.21it/s]#033[A\n",
      "#015 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 22/32 [00:06<00:03,  3.18it/s]#033[A\n",
      "#015 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 23/32 [00:06<00:02,  3.19it/s]#033[A\n",
      "#015 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 24/32 [00:07<00:02,  3.20it/s]#033[A\n",
      "#015 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 25/32 [00:07<00:02,  3.20it/s]#033[A\n",
      "#015 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 26/32 [00:07<00:01,  3.18it/s]#033[A\n",
      "#015 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 27/32 [00:08<00:01,  3.18it/s]#033[A\n",
      "#015 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 28/32 [00:08<00:01,  3.19it/s]#033[A\n",
      "#015 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 29/32 [00:08<00:00,  3.19it/s]#033[A\n",
      "#015 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 30/32 [00:09<00:00,  3.19it/s]#033[A\n",
      "#015 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 31/32 [00:09<00:00,  3.17it/s]#033[A#015                                                 #015\n",
      "#015                                               #015#033[A#015100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [02:59<00:00,  2.95it/s]\n",
      "#015100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:09<00:00,  3.17it/s]#033[A\n",
      "#015                                               #033[A#015                                                 #015#015100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [03:02<00:00,  2.95it/s]#015100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500/500 [03:02<00:00,  2.75it/s]\n",
      "#015  0%|          | 0/32 [00:00<?, ?it/s]#015  6%|‚ñã         | 2/32 [00:00<00:04,  6.25it/s]#015  9%|‚ñâ         | 3/32 [00:00<00:06,  4.80it/s]#015 12%|‚ñà‚ñé        | 4/32 [00:00<00:06,  4.18it/s]#015 16%|‚ñà‚ñå        | 5/32 [00:01<00:07,  3.81it/s]#015 19%|‚ñà‚ñâ        | 6/32 [00:01<00:07,  3.60it/s]#015 22%|‚ñà‚ñà‚ñè       | 7/32 [00:01<00:07,  3.47it/s]#015 25%|‚ñà‚ñà‚ñå       | 8/32 [00:02<00:07,  3.39it/s]#015 28%|‚ñà‚ñà‚ñä       | 9/32 [00:02<00:06,  3.33it/s]#015 31%|‚ñà‚ñà‚ñà‚ñè      | 10/32 [00:02<00:06,  3.26it/s]#015 34%|‚ñà‚ñà‚ñà‚ñç      | 11/32 [00:03<00:06,  3.21it/s]#015 38%|‚ñà‚ñà‚ñà‚ñä      | 12/32 [00:03<00:06,  3.17it/s]#015 41%|‚ñà‚ñà‚ñà‚ñà      | 13/32 [00:03<00:06,  3.15it/s]#015 44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 14/32 [00:04<00:05,  3.17it/s]#015 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 15/32 [00:04<00:05,  3.18it/s]#015 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 16/32 [00:04<00:05,  3.19it/s]#015 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 17/32 [00:05<00:04,  3.19it/s]#015 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 18/32 [00:05<00:04,  3.19it/s]#015 59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 19/32 [00:05<00:04,  3.20it/s]#015 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 20/32 [00:06<00:03,  3.18it/s]#015 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 21/32 [00:06<00:03,  3.16it/s]#015 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 22/32 [00:06<00:03,  3.15it/s]#015 72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 23/32 [00:06<00:02,  3.14it/s]#015 75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 24/32 [00:07<00:02,  3.16it/s]#015 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 25/32 [00:07<00:02,  3.17it/s]#015 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 26/32 [00:07<00:01,  3.19it/s]#015 84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 27/32 [00:08<00:01,  3.19it/s]#015 88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 28/32 [00:08<00:01,  3.19it/s]#015 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 29/32 [00:08<00:00,  3.19it/s]#015 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 30/32 [00:09<00:00,  3.18it/s]#015 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 31/32 [00:09<00:00,  3.18it/s]#015100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [00:09<00:00,  3.34it/s]\n",
      "\n",
      "2022-09-22 21:24:09 Uploading - Uploading generated training model\n",
      "2022-09-22 21:26:09 Completed - Training job completed\n",
      "ProfilerReport-1663881222: NoIssuesFound\n",
      "Training seconds: 574\n",
      "Billable seconds: 574\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# starting the train job with our uploaded datasets as input\n",
    "huggingface_estimator.fit(data, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following diagram shows how a model is trained and deployed with Amazon SageMaker:\n",
    "![](../imgs/sagemaker-platform.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "To deploy our endpoint, we call `deploy()` on our HuggingFace estimator object, passing in our desired number of instances and instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_estimator.deploy(1,\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the returned predictor object to call the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'fear', 'score': 0.9640436172485352}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_input= {\"inputs\": \"I get so nervous before a demo\"}\n",
    "\n",
    "predictor.predict(sentiment_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we delete the inference endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mlops-course')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c4fb925a110d85e819dd5dc868309bd78e2fba0873737123575adeabd3b73abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
